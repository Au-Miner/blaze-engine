=====171
=====119
=====214
=====170
org.apache.spark.BlazeSparkSessionExtension enabled
=====172
=====45
wqlnb: 准备插入策略
原SparkPlan为：
SetCatalogAndNamespace org.apache.spark.sql.connector.catalog.CatalogManager@37d8562f, spark_catalog, ArrayBuffer(default)

-------------
=====41
=====14
=====162
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
Blaze convert strategy for current stage:
=====169
 + SetCatalogAndNamespace (convertible=false, strategy=NeverConvert)
=====13
=====42
Blaze convert result for current stage:
=====169
 + SetCatalogAndNamespace (convertible=false, strategy=NeverConvert)
Transformed spark plan after preColumnarTransitions:
SetCatalogAndNamespace org.apache.spark.sql.connector.catalog.CatalogManager@37d8562f, spark_catalog, ArrayBuffer(default)

转换后SparkPlan为：
SetCatalogAndNamespace org.apache.spark.sql.connector.catalog.CatalogManager@37d8562f, spark_catalog, ArrayBuffer(default)

-------------
=====45
wqlnb: 准备插入策略
原SparkPlan为：
CommandResult <empty>
   +- SetCatalogAndNamespace org.apache.spark.sql.connector.catalog.CatalogManager@37d8562f, spark_catalog, ArrayBuffer(default)

-------------
=====41
=====14
=====162
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
Blaze convert strategy for current stage:
=====169
 + CommandResult (convertible=false, strategy=NeverConvert)
=====13
=====42
Blaze convert result for current stage:
=====169
 + CommandResult (convertible=false, strategy=NeverConvert)
Transformed spark plan after preColumnarTransitions:
CommandResult <empty>
   +- SetCatalogAndNamespace org.apache.spark.sql.connector.catalog.CatalogManager@37d8562f, spark_catalog, ArrayBuffer(default)

转换后SparkPlan为：
CommandResult <empty>
   +- SetCatalogAndNamespace org.apache.spark.sql.connector.catalog.CatalogManager@37d8562f, spark_catalog, ArrayBuffer(default)

-------------
=====213
=====118
=====45
wqlnb: 准备插入策略
原SparkPlan为：
AdaptiveSparkPlan isFinalPlan=false
+- LocalTableScan <empty>

-------------
=====41
=====14
=====162
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
Blaze convert strategy for current stage:
=====169
 + AdaptiveSparkPlan (convertible=false, strategy=NeverConvert)
=====13
=====42
Blaze convert result for current stage:
=====169
 + AdaptiveSparkPlan (convertible=false, strategy=NeverConvert)
Transformed spark plan after preColumnarTransitions:
AdaptiveSparkPlan isFinalPlan=false
+- LocalTableScan <empty>

转换后SparkPlan为：
AdaptiveSparkPlan isFinalPlan=false
+- LocalTableScan <empty>

-------------
=====45
=====45
wqlnb: 准备插入策略
原SparkPlan为：
CommandResult <empty>
   +- AdaptiveSparkPlan isFinalPlan=true
      +- LocalTableScan <empty>

-------------
=====41
=====14
=====162
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
Blaze convert strategy for current stage:
=====169
 + CommandResult (convertible=false, strategy=NeverConvert)
=====13
=====42
Blaze convert result for current stage:
=====169
 + CommandResult (convertible=false, strategy=NeverConvert)
Transformed spark plan after preColumnarTransitions:
CommandResult <empty>
   +- AdaptiveSparkPlan isFinalPlan=true
      +- LocalTableScan <empty>

转换后SparkPlan为：
CommandResult <empty>
   +- AdaptiveSparkPlan isFinalPlan=true
      +- LocalTableScan <empty>

-------------
=====45
wqlnb: 准备插入策略
原SparkPlan为：
DropNamespace V2SessionCatalog(spark_catalog), [test_db], true, false

-------------
=====41
=====14
=====162
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
Blaze convert strategy for current stage:
=====169
 + DropNamespace (convertible=false, strategy=NeverConvert)
=====13
=====42
Blaze convert result for current stage:
=====169
 + DropNamespace (convertible=false, strategy=NeverConvert)
Transformed spark plan after preColumnarTransitions:
DropNamespace V2SessionCatalog(spark_catalog), [test_db], true, false

转换后SparkPlan为：
DropNamespace V2SessionCatalog(spark_catalog), [test_db], true, false

-------------
=====45
wqlnb: 准备插入策略
原SparkPlan为：
CommandResult <empty>
   +- DropNamespace V2SessionCatalog(spark_catalog), [test_db], true, false

-------------
=====41
=====14
=====162
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
Blaze convert strategy for current stage:
=====169
 + CommandResult (convertible=false, strategy=NeverConvert)
=====13
=====42
Blaze convert result for current stage:
=====169
 + CommandResult (convertible=false, strategy=NeverConvert)
Transformed spark plan after preColumnarTransitions:
CommandResult <empty>
   +- DropNamespace V2SessionCatalog(spark_catalog), [test_db], true, false

转换后SparkPlan为：
CommandResult <empty>
   +- DropNamespace V2SessionCatalog(spark_catalog), [test_db], true, false

-------------
=====45
wqlnb: 准备插入策略
原SparkPlan为：
CreateNamespace V2SessionCatalog(spark_catalog), [test_db], false

-------------
=====41
=====14
=====162
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
Blaze convert strategy for current stage:
=====169
 + CreateNamespace (convertible=false, strategy=NeverConvert)
=====13
=====42
Blaze convert result for current stage:
=====169
 + CreateNamespace (convertible=false, strategy=NeverConvert)
Transformed spark plan after preColumnarTransitions:
CreateNamespace V2SessionCatalog(spark_catalog), [test_db], false

转换后SparkPlan为：
CreateNamespace V2SessionCatalog(spark_catalog), [test_db], false

-------------
=====45
wqlnb: 准备插入策略
原SparkPlan为：
CommandResult <empty>
   +- CreateNamespace V2SessionCatalog(spark_catalog), [test_db], false

-------------
=====41
=====14
=====162
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
Blaze convert strategy for current stage:
=====169
 + CommandResult (convertible=false, strategy=NeverConvert)
=====13
=====42
Blaze convert result for current stage:
=====169
 + CommandResult (convertible=false, strategy=NeverConvert)
Transformed spark plan after preColumnarTransitions:
CommandResult <empty>
   +- CreateNamespace V2SessionCatalog(spark_catalog), [test_db], false

转换后SparkPlan为：
CommandResult <empty>
   +- CreateNamespace V2SessionCatalog(spark_catalog), [test_db], false

-------------
=====45
wqlnb: 准备插入策略
原SparkPlan为：
SetCatalogAndNamespace org.apache.spark.sql.connector.catalog.CatalogManager@37d8562f, spark_catalog, ArrayBuffer(test_db)

-------------
=====41
=====14
=====162
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
Blaze convert strategy for current stage:
=====169
 + SetCatalogAndNamespace (convertible=false, strategy=NeverConvert)
=====13
=====42
Blaze convert result for current stage:
=====169
 + SetCatalogAndNamespace (convertible=false, strategy=NeverConvert)
Transformed spark plan after preColumnarTransitions:
SetCatalogAndNamespace org.apache.spark.sql.connector.catalog.CatalogManager@37d8562f, spark_catalog, ArrayBuffer(test_db)

转换后SparkPlan为：
SetCatalogAndNamespace org.apache.spark.sql.connector.catalog.CatalogManager@37d8562f, spark_catalog, ArrayBuffer(test_db)

-------------
=====45
wqlnb: 准备插入策略
原SparkPlan为：
CommandResult <empty>
   +- SetCatalogAndNamespace org.apache.spark.sql.connector.catalog.CatalogManager@37d8562f, spark_catalog, ArrayBuffer(test_db)

-------------
=====41
=====14
=====162
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
Blaze convert strategy for current stage:
=====169
 + CommandResult (convertible=false, strategy=NeverConvert)
=====13
=====42
Blaze convert result for current stage:
=====169
 + CommandResult (convertible=false, strategy=NeverConvert)
Transformed spark plan after preColumnarTransitions:
CommandResult <empty>
   +- SetCatalogAndNamespace org.apache.spark.sql.connector.catalog.CatalogManager@37d8562f, spark_catalog, ArrayBuffer(test_db)

转换后SparkPlan为：
CommandResult <empty>
   +- SetCatalogAndNamespace org.apache.spark.sql.connector.catalog.CatalogManager@37d8562f, spark_catalog, ArrayBuffer(test_db)

-------------
=====213
=====118
=====45
wqlnb: 准备插入策略
原SparkPlan为：
AdaptiveSparkPlan isFinalPlan=false
+- LocalTableScan <empty>

-------------
=====41
=====14
=====162
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
Blaze convert strategy for current stage:
=====169
 + AdaptiveSparkPlan (convertible=false, strategy=NeverConvert)
=====13
=====42
Blaze convert result for current stage:
=====169
 + AdaptiveSparkPlan (convertible=false, strategy=NeverConvert)
Transformed spark plan after preColumnarTransitions:
AdaptiveSparkPlan isFinalPlan=false
+- LocalTableScan <empty>

转换后SparkPlan为：
AdaptiveSparkPlan isFinalPlan=false
+- LocalTableScan <empty>

-------------
=====45
=====45
wqlnb: 准备插入策略
原SparkPlan为：
CommandResult <empty>
   +- AdaptiveSparkPlan isFinalPlan=true
      +- LocalTableScan <empty>

-------------
=====41
=====14
=====162
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
Blaze convert strategy for current stage:
=====169
 + CommandResult (convertible=false, strategy=NeverConvert)
=====13
=====42
Blaze convert result for current stage:
=====169
 + CommandResult (convertible=false, strategy=NeverConvert)
Transformed spark plan after preColumnarTransitions:
CommandResult <empty>
   +- AdaptiveSparkPlan isFinalPlan=true
      +- LocalTableScan <empty>

转换后SparkPlan为：
CommandResult <empty>
   +- AdaptiveSparkPlan isFinalPlan=true
      +- LocalTableScan <empty>

-------------
=====45
wqlnb: 准备插入策略
原SparkPlan为：
Execute CreateDataSourceTableCommand
   +- CreateDataSourceTableCommand `test_db`.`test_table`, true

-------------
=====41
=====14
=====162
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
Blaze convert strategy for current stage:
=====169
 + Execute CreateDataSourceTableCommand (convertible=false, strategy=NeverConvert)
=====13
=====42
Blaze convert result for current stage:
=====169
 + Execute CreateDataSourceTableCommand (convertible=false, strategy=NeverConvert)
Transformed spark plan after preColumnarTransitions:
Execute CreateDataSourceTableCommand
   +- CreateDataSourceTableCommand `test_db`.`test_table`, true

转换后SparkPlan为：
Execute CreateDataSourceTableCommand
   +- CreateDataSourceTableCommand `test_db`.`test_table`, true

-------------
=====45
wqlnb: 准备插入策略
原SparkPlan为：
CommandResult <empty>
   +- Execute CreateDataSourceTableCommand
         +- CreateDataSourceTableCommand `test_db`.`test_table`, true

-------------
=====41
=====14
=====162
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
Blaze convert strategy for current stage:
=====169
 + CommandResult (convertible=false, strategy=NeverConvert)
=====13
=====42
Blaze convert result for current stage:
=====169
 + CommandResult (convertible=false, strategy=NeverConvert)
Transformed spark plan after preColumnarTransitions:
CommandResult <empty>
   +- Execute CreateDataSourceTableCommand
         +- CreateDataSourceTableCommand `test_db`.`test_table`, true

转换后SparkPlan为：
CommandResult <empty>
   +- Execute CreateDataSourceTableCommand
         +- CreateDataSourceTableCommand `test_db`.`test_table`, true

-------------
=====213
=====118
=====45
wqlnb: 准备插入策略
原SparkPlan为：
Execute InsertIntoHadoopFsRelationCommand file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table, false, Parquet, [path=file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table], Append, CatalogTable(
Database: test_db
Table: test_table
Owner: au_miner
Created Time: Wed Jul 10 15:36:44 CST 2024
Last Access: UNKNOWN
Created By: Spark 3.3.3
Type: MANAGED
Provider: PARQUET
Location: file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table
Serde Library: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
Schema: root
 |-- id: integer (nullable = true)
 |-- value: double (nullable = true)
), org.apache.spark.sql.execution.datasources.InMemoryFileIndex@7b393942, [id, value]
+- AdaptiveSparkPlan isFinalPlan=false
   +- LocalTableScan [id#4, value#5]

-------------
=====41
=====14
=====162
=====142
=====64
=====142
=====14
=====162
=====142
=====64
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
Blaze convert strategy for current stage:
=====169
 + Execute InsertIntoHadoopFsRelationCommand (convertible=false, strategy=NeverConvert)
=====169
 +- AdaptiveSparkPlan (convertible=false, strategy=NeverConvert)
=====13
=====42
=====42
Blaze convert result for current stage:
=====169
 + Execute InsertIntoHadoopFsRelationCommand (convertible=false, strategy=NeverConvert)
=====169
 +- AdaptiveSparkPlan (convertible=false, strategy=NeverConvert)
Transformed spark plan after preColumnarTransitions:
Execute InsertIntoHadoopFsRelationCommand file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table, false, Parquet, [path=file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table], Append, CatalogTable(
Database: test_db
Table: test_table
Owner: au_miner
Created Time: Wed Jul 10 15:36:44 CST 2024
Last Access: UNKNOWN
Created By: Spark 3.3.3
Type: MANAGED
Provider: PARQUET
Location: file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table
Serde Library: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
Schema: root
 |-- id: integer (nullable = true)
 |-- value: double (nullable = true)
), org.apache.spark.sql.execution.datasources.InMemoryFileIndex@7b393942, [id, value]
+- AdaptiveSparkPlan isFinalPlan=false
   +- LocalTableScan [id#4, value#5]

转换后SparkPlan为：
Execute InsertIntoHadoopFsRelationCommand file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table, false, Parquet, [path=file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table], Append, CatalogTable(
Database: test_db
Table: test_table
Owner: au_miner
Created Time: Wed Jul 10 15:36:44 CST 2024
Last Access: UNKNOWN
Created By: Spark 3.3.3
Type: MANAGED
Provider: PARQUET
Location: file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table
Serde Library: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
Schema: root
 |-- id: integer (nullable = true)
 |-- value: double (nullable = true)
), org.apache.spark.sql.execution.datasources.InMemoryFileIndex@7b393942, [id, value]
+- AdaptiveSparkPlan isFinalPlan=false
   +- LocalTableScan [id#4, value#5]

-------------
=====45
=====45
wqlnb: 准备插入策略
原SparkPlan为：
CommandResult <empty>
   +- Execute InsertIntoHadoopFsRelationCommand file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table, false, Parquet, [path=file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table], Append, CatalogTable(
Database: test_db
Table: test_table
Owner: au_miner
Created Time: Wed Jul 10 15:36:44 CST 2024
Last Access: UNKNOWN
Created By: Spark 3.3.3
Type: MANAGED
Provider: PARQUET
Location: file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table
Serde Library: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
Schema: root
 |-- id: integer (nullable = true)
 |-- value: double (nullable = true)
), org.apache.spark.sql.execution.datasources.InMemoryFileIndex@7b393942, [id, value]
      +- AdaptiveSparkPlan isFinalPlan=true
         +- LocalTableScan [id#4, value#5]

-------------
=====41
=====14
=====162
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
Blaze convert strategy for current stage:
=====169
 + CommandResult (convertible=false, strategy=NeverConvert)
=====13
=====42
Blaze convert result for current stage:
=====169
 + CommandResult (convertible=false, strategy=NeverConvert)
Transformed spark plan after preColumnarTransitions:
CommandResult <empty>
   +- Execute InsertIntoHadoopFsRelationCommand file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table, false, Parquet, [path=file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table], Append, CatalogTable(
Database: test_db
Table: test_table
Owner: au_miner
Created Time: Wed Jul 10 15:36:44 CST 2024
Last Access: UNKNOWN
Created By: Spark 3.3.3
Type: MANAGED
Provider: PARQUET
Location: file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table
Serde Library: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
Schema: root
 |-- id: integer (nullable = true)
 |-- value: double (nullable = true)
), org.apache.spark.sql.execution.datasources.InMemoryFileIndex@7b393942, [id, value]
      +- AdaptiveSparkPlan isFinalPlan=true
         +- LocalTableScan [id#4, value#5]

转换后SparkPlan为：
CommandResult <empty>
   +- Execute InsertIntoHadoopFsRelationCommand file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table, false, Parquet, [path=file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table], Append, CatalogTable(
Database: test_db
Table: test_table
Owner: au_miner
Created Time: Wed Jul 10 15:36:44 CST 2024
Last Access: UNKNOWN
Created By: Spark 3.3.3
Type: MANAGED
Provider: PARQUET
Location: file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spark-warehouse/test_db.db/test_table
Serde Library: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
Schema: root
 |-- id: integer (nullable = true)
 |-- value: double (nullable = true)
), org.apache.spark.sql.execution.datasources.InMemoryFileIndex@7b393942, [id, value]
      +- AdaptiveSparkPlan isFinalPlan=true
         +- LocalTableScan [id#4, value#5]

-------------
=====213
=====118
=====118
=====118
=====118
=====45
wqlnb: 准备插入策略
原SparkPlan为：
AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[], functions=[avg(value#7)], output=[avg(value)#9])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=67]
      +- HashAggregate(keys=[], functions=[partial_avg(value#7)], output=[sum#14, count#15L])
         +- FileScan parquet test_db.test_table[value#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:double>

-------------
=====41
=====14
=====162
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
Blaze convert strategy for current stage:
=====169
 + AdaptiveSparkPlan (convertible=false, strategy=NeverConvert)
=====13
=====42
Blaze convert result for current stage:
=====169
 + AdaptiveSparkPlan (convertible=false, strategy=NeverConvert)
Transformed spark plan after preColumnarTransitions:
AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[], functions=[avg(value#7)], output=[avg(value)#9])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=67]
      +- HashAggregate(keys=[], functions=[partial_avg(value#7)], output=[sum#14, count#15L])
         +- FileScan parquet test_db.test_table[value#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:double>

转换后SparkPlan为：
AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[], functions=[avg(value#7)], output=[avg(value)#9])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=67]
      +- HashAggregate(keys=[], functions=[partial_avg(value#7)], output=[sum#14, count#15L])
         +- FileScan parquet test_db.test_table[value#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:double>

-------------
=====45
wqlnb: 准备插入策略
原SparkPlan为：
Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=67]
+- HashAggregate(keys=[], functions=[partial_avg(value#7)], output=[sum#14, count#15L])
   +- FileScan parquet test_db.test_table[value#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:double>

-------------
=====41
=====14
=====15
=====17
=====148
Converting FileSourceScanExec: Scan parquet test_db.test_table (unknown)
  relation: parquet
  relation.location: org.apache.spark.sql.execution.datasources.InMemoryFileIndex@7b393942
  output: List(value#7)
  requiredSchema: StructType(StructField(value,DoubleType,true))
  partitionFilters: Stream()
  optionalBucketSet: None
  dataFilters: List()
  tableIdentifier: Some(`test_db`.`test_table`)
=====131
=====173
=====174
=====175
=====53
=====52
=====50
=====52
=====50
=====176
=====53
=====192
=====39
=====38
=====133
=====64
=====142
=====14
=====15
=====29
=====40
=====148
Converting HashAggregateExec: HashAggregate (unknown)
=====37
=====64
=====142
=====39
=====38
=====64
=====142
=====65
=====143
=====121
=====215
=====223
=====218
=====216
=====96
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====57
=====219
=====220
=====218
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====57
=====222
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====57
=====227
=====193
=====64
=====142
=====14
=====15
=====16
=====148
Converting ShuffleExchangeExec: Exchange (unknown)
=====37
=====64
=====142
=====39
=====38
=====194
=====64
=====142
=====65
=====143
=====194
=====194
=====134
=====179
=====194
=====68
=====194
=====53
=====52
=====50
=====180
=====183
=====196
=====194
=====64
=====142
=====64
=====142
=====64
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====43
=====42
=====43
=====43
=====42
=====43
=====43
Blaze convert strategy for current stage:
=====169
 + Exchange (convertible=true, strategy=AlwaysConvert)
=====169
 +- HashAggregate (convertible=true, strategy=AlwaysConvert)
=====169
 +-- Scan parquet test_db.test_table (convertible=true, strategy=AlwaysConvert)
=====13
=====42
=====14
=====15
=====17
=====148
Converting FileSourceScanExec: Scan parquet test_db.test_table (unknown)
  relation: parquet
  relation.location: org.apache.spark.sql.execution.datasources.InMemoryFileIndex@7b393942
  output: List(value#7)
  requiredSchema: StructType(StructField(value,DoubleType,true))
  partitionFilters: Stream()
  optionalBucketSet: None
  dataFilters: List()
  tableIdentifier: Some(`test_db`.`test_table`)
=====131
=====173
=====174
=====175
=====53
=====52
=====50
=====52
=====50
=====176
=====53
=====192
=====39
=====38
=====133
=====42
=====14
=====15
=====29
=====40
=====148
Converting HashAggregateExec: HashAggregate (unknown)
=====37
=====64
=====142
=====39
=====38
=====64
=====142
=====65
=====143
=====121
=====215
=====223
=====218
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====57
=====219
=====220
=====218
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====57
=====222
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====57
=====227
=====193
=====42
=====14
=====15
=====16
=====148
Converting ShuffleExchangeExec: Exchange (unknown)
=====37
=====64
=====142
=====39
=====38
=====194
=====64
=====142
=====65
=====143
=====194
=====194
=====134
=====179
=====194
=====68
=====194
=====53
=====52
=====50
=====180
=====183
=====196
=====194
Blaze convert result for current stage:
=====169
 + NativeShuffleExchange (convertible=false, strategy=Default)
=====169
 +- NativeHashAggregate (convertible=false, strategy=Default)
=====169
 +-- InputAdapter (convertible=false, strategy=Default)
=====169
 +--- NativeParquetScan test_db.test_table (convertible=false, strategy=Default)
=====194
=====196
=====194
Transformed spark plan after preColumnarTransitions:
NativeShuffleExchange SinglePartition, [plan_id=84]
+- NativeHashAggregate HashAgg, [partial_avg(value#7)], [sum#12, count#13L], 0
   +- InputAdapter [#7]
      +- NativeParquetScan test_db.test_table (FileScan parquet test_db.test_table[value#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:double>)

=====194
转换后SparkPlan为：
NativeShuffleExchange SinglePartition, [plan_id=84]
+- NativeHashAggregate HashAgg, [partial_avg(value#7)], [sum#12, count#13L], 0
   +- InputAdapter [#7]
      +- NativeParquetScan test_db.test_table (FileScan parquet test_db.test_table[value#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:double>)

-------------
=====194
发生了转换
，转换前为：
Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=67]
+- HashAggregate(keys=[], functions=[partial_avg(value#7)], output=[sum#14, count#15L])
   +- FileScan parquet test_db.test_table[value#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:double>

，转换后为：
NativeShuffleExchange SinglePartition, [plan_id=84]
+- NativeHashAggregate HashAgg, [partial_avg(value#7)], [sum#12, count#13L], 0
   +- InputAdapter [#7]
      +- NativeParquetScan test_db.test_table (FileScan parquet test_db.test_table[value#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:double>)

-------------
=====194
=====194
=====194
=====194
=====215
=====223
=====218
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====57
=====219
=====220
=====218
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====57
=====222
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====57
=====227
=====193
=====195
=====215
=====223
=====218
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====57
=====219
=====220
=====218
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====57
=====222
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====57
=====227
=====193
=====191
=====179
=====194
=====68
=====194
=====53
=====52
=====50
=====180
=====183
=====196
=====194
=====196
=====194
=====194
=====194
=====194
=====194
=====194
=====68
=====68
wqlnb: 真的走这里了！！！！！！！！
=====64
=====142
=====66
=====144
=====70
=====95
=====66
=====144
=====70
=====66
=====144
=====70
=====178
=====174
=====175
=====53
=====52
=====50
=====52
=====50
=====176
=====53
=====163
=====151
=====150
=====151
=====217
=====221
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====57
=====220
=====222
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====57
=====218
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====57
=====219
=====150
=====151
=====194
=====182
=====180
=====150
=====151
=====189
=====111
=====112
=====113
=====177
=====156
=====67
=====48
=====47
=====48
=====47
=====48
=====47
=====48
=====47
=====12
Initializing native environment (batchSize=10000, nativeMemory=521666560, memoryFraction=0.6
libName的内容为：libblaze.dylib
classLoader.getResourceAsStream(libName)：sun.net.www.protocol.jar.JarURLConnection$JarURLInputStream@14f4306a
wqlnb: 目前正在查找的tempFile.getAbsolutePath内容为：/var/folders/ww/c6lyr1sn3j5fd1sp60_bz_r40000gn/T/libblaze-3226334665357315681.tmp
Start executing native plan
=====1
=====197
=====198
=====10
=====6
=====75
=====76
=====82
=====80
=====78
=====201
=====4
=====198
=====198
=====198
=====198
=====198
=====198
=====198
=====202
=====198
=====198
=====202
=====198
=====198
=====202
=====198
=====202
=====202
=====198
=====198
=====202
=====202
=====198
=====198
=====202
=====200
=====198
=====198
=====202
=====5
=====9
=====2
=====208
=====208
=====199
=====199
=====11
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====46
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====46
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====46
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====9
=====153
=====11
=====213
=====118
=====118
=====190
=====45
wqlnb: 准备插入策略
=====194
原SparkPlan为：
HashAggregate(keys=[], functions=[avg(value#7)], output=[avg(value)#9])
+- ShuffleQueryStage 0
   +- NativeShuffleExchange SinglePartition, [plan_id=84]
      +- NativeHashAggregate HashAgg, [partial_avg(value#7)], [sum#12, count#13L], 0
         +- InputAdapter [#7]
            +- NativeParquetScan test_db.test_table (FileScan parquet test_db.test_table[value#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:double>)

-------------
=====41
=====14
=====162
=====142
=====142
=====64
=====142
=====142
=====14
=====15
=====29
=====40
=====148
Converting HashAggregateExec: HashAggregate (unknown)
=====99
=====145
=====147
=====38
=====194
=====64
=====142
=====142
=====65
=====143
=====143
=====194
=====194
=====37
=====64
=====142
=====142
=====121
=====215
=====223
=====218
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====219
=====220
=====218
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====222
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====227
=====193
=====132
=====55
=====58
=====57
=====64
=====142
=====64
=====142
=====142
=====64
=====142
=====44
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====42
=====43
=====42
=====43
=====43
Blaze convert strategy for current stage:
=====169
 + HashAggregate (convertible=true, strategy=AlwaysConvert)
=====169
 +- ShuffleQueryStage (convertible=true, strategy=AlwaysConvert)
=====13
=====42
=====14
=====162
=====142
=====142
=====42
=====14
=====15
=====29
=====40
=====148
Converting HashAggregateExec: HashAggregate (unknown)
=====99
=====145
=====147
=====38
=====194
=====64
=====142
=====142
=====65
=====143
=====143
=====194
=====194
=====37
=====64
=====142
=====142
=====121
=====215
=====223
=====218
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====219
=====220
=====218
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====222
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====227
=====193
=====132
=====55
=====58
=====57
Blaze convert result for current stage:
=====169
 + NativeProject (convertible=false, strategy=Default)
=====169
 +- NativeHashAggregate (convertible=false, strategy=Default)
=====169
 +-- ShuffleQueryStage (convertible=true, strategy=AlwaysConvert)
=====194
=====196
=====194
=====194
=====194
=====194
Transformed spark plan after preColumnarTransitions:
NativeProject [avg(value#7)#8 AS avg(value)#9], false
+- NativeHashAggregate HashAgg, List(), [avg(value#7)], [avg(value#7)#8], 0
   +- ShuffleQueryStage 0
      +- NativeShuffleExchange SinglePartition, [plan_id=84]
         +- NativeHashAggregate HashAgg, [partial_avg(value#7)], [sum#12, count#13L], 0
            +- InputAdapter [#7]
               +- NativeParquetScan test_db.test_table (FileScan parquet test_db.test_table[value#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:double>)

=====194
=====194
=====194
转换后SparkPlan为：
NativeProject [avg(value#7)#8 AS avg(value)#9], false
+- NativeHashAggregate HashAgg, List(), [avg(value#7)], [avg(value#7)#8], 0
   +- ShuffleQueryStage 0
      +- NativeShuffleExchange SinglePartition, [plan_id=84]
         +- NativeHashAggregate HashAgg, [partial_avg(value#7)], [sum#12, count#13L], 0
            +- InputAdapter [#7]
               +- NativeParquetScan test_db.test_table (FileScan parquet test_db.test_table[value#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:double>)

-------------
=====194
=====194
=====194
=====194
发生了转换
，转换前为：
HashAggregate(keys=[], functions=[avg(value#7)], output=[avg(value)#9])
+- ShuffleQueryStage 0
   +- NativeShuffleExchange SinglePartition, [plan_id=84]
      +- NativeHashAggregate HashAgg, [partial_avg(value#7)], [sum#12, count#13L], 0
         +- InputAdapter [#7]
            +- NativeParquetScan test_db.test_table (FileScan parquet test_db.test_table[value#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:double>)

，转换后为：
NativeProject [avg(value#7)#8 AS avg(value)#9], false
+- NativeHashAggregate HashAgg, List(), [avg(value#7)], [avg(value#7)#8], 0
   +- ShuffleQueryStage 0
      +- NativeShuffleExchange SinglePartition, [plan_id=84]
         +- NativeHashAggregate HashAgg, [partial_avg(value#7)], [sum#12, count#13L], 0
            +- InputAdapter [#7]
               +- NativeParquetScan test_db.test_table (FileScan parquet test_db.test_table[value#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/au_miner/opt/workspace/project/ZXProject/blaze-engine/spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:double>)

-------------
=====194
=====194
=====194
=====194
=====187
=====196
=====194
=====194
=====194
=====194
=====196
=====194
=====194
=====68
=====194
=====68
=====194
=====194
=====66
=====144
=====70
=====95
=====66
=====144
=====144
=====70
=====181
=====188
=====150
=====151
=====217
=====221
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====220
=====222
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====218
=====216
=====97
=====98
=====50
=====164
=====167
=====59
=====160
=====55
=====58
=====219
=====150
=====151
=====55
=====58
=====57
=====150
=====151
=====69
=====194
=====53
=====52
=====50
=====112
=====109
=====67
=====48
=====47
=====48
=====47
=====48
=====47
Start executing native plan
=====1
=====10
=====6
=====76
=====82
=====80
=====78
=====201
=====4
=====5
=====198
=====198
=====198
=====202
=====198
=====198
=====9
=====2
=====198
=====198
=====202
=====200
=====198
=====198
=====202
=====198
=====198
=====202
=====198
=====202
=====198
=====198
=====202
=====202
=====198
=====198
=====202
=====198
=====198
=====202
=====110
=====152
=====199
=====7
=====76
=====90
=====80
=====78
=====210
=====9
=====3
=====9
wqlnb: 正在获取的batchRow为: [0,403e000000000000]
=====9
=====2
=====11
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====46
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====46
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====47
=====9
=====11
30.0
